{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "source": [
        "# Supervised Model Project\n",
        "\n",
        "## Load the Libraries"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from Feature import *\n",
        "from GUI_models import train_predict\n",
        "from Models import *\n",
        "from Models import AdaBoost, Decision_Tree\n",
        "from Models import GradientBoost, KNN, Linear_Discriminant\n",
        "from Models import Logistic_reg, Naive_Bayes, SVC\n",
        "from Models import Random_Forest, SGD, Quadratic_Discriminant\n",
        "from joblib import load\n",
        "from os.path import dirname, basename, isfile, join\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.base import clone\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import tkinter as tk\n",
        "import pandas as pd\n",
        "from pandastable import Table, TableModel"
      ]
    },
    {
      "source": [
        "## Create the functions, classes, and dictionaries for this notebook"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataFrameTable(tk.Frame):\n",
        "    def __init__(self, parent=None, df=pd.DataFrame()):\n",
        "        super().__init__()\n",
        "        self.parent = parent\n",
        "        self.pack(fill=tk.BOTH, expand=True)\n",
        "        self.table = Table(\n",
        "            self, dataframe=df,\n",
        "            showtoolbar=False,\n",
        "            showstatusbar=True,\n",
        "            editable=False)\n",
        "        self.table.show()\n",
        "\n",
        "\n",
        "def _quit():\n",
        "    root.quit()\n",
        "    root.destroy()\n",
        "\n",
        "\n",
        "def _stop():\n",
        "    \"\"\"Stop scanning by setting the global flag to False.\"\"\"\n",
        "    global running\n",
        "    root.quit()\n",
        "    root.destroy()\n",
        "    running = False\n",
        "\n",
        "\n",
        "fun_dict = {\n",
        "    'AdaBoost': AdaBoost.train_AdaBoost,\n",
        "    'Decision_Tree': Decision_Tree.train_Decision_Tree,\n",
        "    'GradientBoost': GradientBoost.train_GradientBoost,\n",
        "    'KNN': KNN.train_KNN,\n",
        "    'Linear_Discriminant':\n",
        "        Linear_Discriminant.train_Linear_Discriminant,\n",
        "    'Logistic_reg': Logistic_reg.train_Logistic_reg,\n",
        "    'Naive_Bayes': Naive_Bayes.train_Naive_Bayes,\n",
        "    'SVC': SVC.train_SVC,\n",
        "    'Random_Forest': Random_Forest.train_Random_Forest,\n",
        "    'SGD': SGD.train_SGD,\n",
        "    'Quadratic_Discriminant':\n",
        "        Quadratic_Discriminant.train_Quadratic_Discriminant\n",
        "}"
      ]
    },
    {
      "source": [
        "## Load the Data"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('Data_For_Model.csv')\n",
        "X = df.iloc[:, 1::].values\n",
        "y = df.iloc[:, 0].values\n",
        "# Split the data into train and validation. The training set will be\n",
        "# used later in k-cross validation, so it remains as X, and y\n",
        "X_train, X_validation, y_train, y_validation = \\\n",
        "    train_test_split(X, y, test_size=0.3,\n",
        "                     random_state=42)"
      ]
    },
    {
      "source": [
        "## Run the GUI, where it asks whether the user wants to train, predict or check the performance of the current classifiers"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "running = True\n",
        "\n",
        "while running:\n",
        "\n",
        "    action = train_predict.train_predict_check()\n",
        "\n",
        "    if action == 1:\n",
        "        # Load the classification models\n",
        "        classifiers = train_predict.train()\n",
        "        clf_dict = {}\n",
        "        for clf in classifiers:\n",
        "            clf_dict[clf] = fun_dict[clf](X_train, y_train)\n",
        "\n",
        "        cols = ['Classifier', 'Accuracy', 'Precision', 'Recall', 'F1Score']\n",
        "        df_metrics = pd.DataFrame(columns=cols)\n",
        "        clfs = list(fun_dict.keys())\n",
        "\n",
        "        for clf in clfs:\n",
        "\n",
        "            model = load(f'Models/{clf}_model.joblib')\n",
        "            y_pred = model.predict(X_validation)\n",
        "            accu = accuracy_score(y_validation, y_pred)\n",
        "            prec = precision_score(y_validation, y_pred, average='weighted')\n",
        "            recall = recall_score(y_validation, y_pred, average='weighted')\n",
        "            f1 = f1_score(y_validation, y_pred, average='weighted')\n",
        "            new_entry = pd.DataFrame([[clf, accu, prec, recall, f1]],\n",
        "                                     columns=cols)\n",
        "            df_metrics = df_metrics.append(new_entry, ignore_index=True)\n",
        "\n",
        "        df_metrics.to_csv('Models/Metrics.csv', index=False)\n",
        "\n",
        "    elif action == 2:\n",
        "        # Get information for the next round\n",
        "        # Ask what league the user wants to predict from:\n",
        "        next_match_list = train_predict.get_next_matches()\n",
        "\n",
        "        # Ask what model the user wants to use for prediction:\n",
        "        clf = train_predict.choose_model()[0]\n",
        "        clf = load(f'Models/{clf}_model.joblib')\n",
        "        leagues = []\n",
        "        last_season = []\n",
        "        for match in next_match_list:\n",
        "            # Take only the leagues and years of the desired predictions\n",
        "            leagues.append(match['League'].unique()[0])\n",
        "            last_season.append(match['Season'].unique()[0])\n",
        "        season_league = zip(last_season, leagues)\n",
        "        df_sta_whole = pd.read_csv('Standings_Cleaned.csv')\n",
        "        df_sta_list = []\n",
        "\n",
        "        for season, league in season_league:\n",
        "            df_sta_list.append(\n",
        "                df_sta_whole[\n",
        "                    (df_sta_whole['Season'] == season)\n",
        "                    & (df_sta_whole['League'] == league)])\n",
        "        df_sta = pd.concat(df_sta_list)\n",
        "\n",
        "        # Let's concat the match list in a single Dataframe\n",
        "        df_pred = pd.concat(next_match_list)\n",
        "        df_pred[['Home_Goals', 'Away_Goals', 'Label']] = None\n",
        "        create_features(df_pred, df_sta, predict=True)\n",
        "        df_to_predict = pd.read_csv('Data_to_Predict.csv')\n",
        "        X_predict = df_to_predict.iloc[:, 1::].values\n",
        "        y_predict = df_to_predict.iloc[:, 0].values\n",
        "        predictions = clf.predict(X_predict)\n",
        "\n",
        "        matches = []\n",
        "        for index, row in df_pred.iterrows():\n",
        "            matches.append(f'{row[\"Home_Team\"]} vs.'\n",
        "                           + f'{row[\"Away_Team\"]}')\n",
        "        df_to_show = pd.DataFrame({'Match': matches,\n",
        "                                   'Predictions': predictions})\n",
        "        root = tk.Tk()\n",
        "        table = DataFrameTable(root, df_to_show)\n",
        "        button = tk.Button(master=root, text=\"Quit\", command=_quit)\n",
        "        button.pack(side=tk.BOTTOM)\n",
        "        root.mainloop()\n",
        "\n",
        "    elif action == 3:\n",
        "\n",
        "        df_metrics = pd.read_csv('Models/Metrics.csv')\n",
        "        root = tk.Tk()\n",
        "        table = DataFrameTable(root, df_metrics)\n",
        "        button_back = tk.Button(root, text='Go Back', command=_quit)\n",
        "        button_back.pack(side=tk.BOTTOM)\n",
        "        button = tk.Button(master=root, text=\"Quit\", command=_stop)\n",
        "        button.pack(side=tk.BOTTOM)\n",
        "        root.mainloop()"
      ]
    },
    {
      "source": [
        "## Graphically, see the performance"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLWUfFnrpbEf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "outputId": "6d286e00-375d-45cc-d362-dee568227848"
      },
      "source": [
        "barWidth = 0.2\n",
        "r1 = np.arange(len(df_metrics.Classifier))\n",
        "r2 = [x + barWidth for x in r1]\n",
        "r3 = [x + barWidth for x in r2]\n",
        "r4 = [x + barWidth for x in r3]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15, 10))\n",
        "plt.bar(r1, df_metrics.Accuracy, width=0.2, label='Accuracy')\n",
        "plt.bar(r2, df_metrics.Precision, width=0.2, tick_label=list(df_metrics.Classifier), label='Precision')\n",
        "plt.xticks(rotation=45)\n",
        "plt.bar(r3, df_metrics.Recall, width=0.2, label='Recall')\n",
        "plt.bar(r4, df_metrics.F1Score, width=0.2, label='F1-Score')\n",
        "plt.ylim([0.0, 0.7])\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## WE can see that the best performance corresponds to  the GradientBoost, so we load it and see other metrics within it"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "zyznOtoj7-SP",
        "outputId": "c07248d0-721f-40ec-ffc7-a862e24eda0d"
      },
      "source": [
        "clf = load('Models/GradientBoost_model.joblib')\n",
        "best_clf = clf.best_estimator_\n",
        "disp = plot_confusion_matrix(best_clf, X_validation, y_validation,\n",
        "                             cmap=plt.cm.Blues)\n",
        "print(disp.confusion_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "features = ['Position_Home_Norm', 'Position_Away_Norm', 'Goals_For_Home_Norm', 'Goals_Against_Home_Norm','Goals_For_Away_Norm', 'Goals_Against_Aways_Norm', 'Round_Norm', 'Home_Streak', 'Away_Streak','Home_Streak_Total', 'Away_Streak_Total', 'Weekend', 'Daytime']\n",
        "feat_importance = best_clf.feature_importances_\n",
        "df_features = pd.DataFrame({'Features': features, 'Importances': feat_importance})\n",
        "df_features = df_features.sort_values('Importances', ascending = False)\n",
        "ax = df_features.plot.barh(x='Features', y='Importances', rot=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_proba = clf.predict_proba(X_validation)\n",
        "y_val_linear = label_binarize(y_validation, classes=[0, 1, 2])\n",
        "n_classes = y_val_linear.shape[1]\n",
        "\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "thresholds = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], thresholds[i] = roc_curve(y_val_linear[:, i], y_proba[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_val_linear.ravel(), y_proba.ravel())\n",
        "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot(fpr[2], tpr[2], color='darkorange',\n",
        "         lw=2, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic example')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "source": [
        "Every point in the ROC curve is plotted with a different threshold. A good way to find a good threshold is getting a balance between TPR and FPR, which can be calculated with the G-Mean"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gmean = tpr[2] * (1-fpr[2])\n",
        "idx = np.argmax(gmean)\n",
        "best_threshold = thresholds[2][idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot(fpr[2], tpr[2], color='darkorange',\n",
        "         lw=2, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.scatter(fpr[2][idx], tpr[2][idx], marker='o', color='black', label=f'Best Threshold = {round(best_threshold,3)}')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}